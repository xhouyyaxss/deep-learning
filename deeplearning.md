[toc]



# 前言

## **如何学好深度学习**

**1. 夯实基础**

- 线性代数、微积分、概率论与数理统计
- 编程语言：Python或R
- 机器学习基础：有监督学习、无监督学习

**2. 选择合适的学习资源**

- 在线课程：Coursera、edX、Udacity
- 书籍：Deep Learning、Machine Learning Yearning
- 博客和文章：Andrej Karpathy、Yann LeCun

**3. 实践动手操作**

- 搭建深度学习环境：TensorFlow、PyTorch、Keras
- 编写代码实现基本深度学习模型
- 参与Kaggle竞赛或个人项目

**4. 理解深度学习原理**

- 卷积神经网络：卷积、池化、激活函数
- 循环神经网络：RNN、LSTM、GRU
- 注意力机制：注意力层、Transformer

**5. 探索不同应用领域**

- 图像识别与分类
- 自然语言处理
- 语音识别与合成
- 医学影像分析

**6. 关注前沿研究**

- 阅读学术论文：ICML、NeurIPS、CVPR
- 参加会议和研讨会
- 探索新的深度学习架构和算法



# Deep Learning

## 机器学习流程

* 数据提取
* 特征工程（作用）

  - 数据特征决定了模型的上限、
  - <mark>预处理和特征提取是最核心的</mark>
    - DL中，卷积神经网络（CNN）使用各种卷积（convolution）提取特征，最后再通过各种激活函数算法进行分类
  - 算法与参数选择决定了如何逼近这个上限
* 建立模型
* 评估与应用

##### 机器学习常规步骤

- 收集数据并给定标签
  - cifar-10 图片集

- 训练一个分类器
- 测试评估

## 训练算法

##### 传统训练方法

* k-邻近不能用于计算机视觉，因为k-邻近也把背景包括进去，我们只关注主体（k-邻近：计算测试数据和其他数据的距离并排序，用给定的k值进行比较，计算k值中每个类别的频率，频率最高的就是目标答案）

##### 神经网络中使用的方法：实质就是寻找最优解的参数是的结果更加的逼近目标值

- **线性函数**：<mark>f(x,W)=wx+b</mark>，其中，w是每个像素的权重，b是偏置，x是一个像素个数乘以1的矩阵，w是一个需要的类别个数乘以像素数的矩阵，结果通常被称为得分函数，计算每个像素在每个类别的得分并每次和正确的对比

- **损失函数**：需要明确的指导模型的当前效果多好或者多差，简单的来说，每个任务的损失函数的不同，参数才能根据该函数进行调整，通过损失函数计算出结果判断损失的程度，我们总是希望模型不要太复杂，过度拟合的模型是没用的（参数在局部生效，不平稳，没有普遍性，没有代表性）<mark>通常用在回归任务，计算得分值</mark>，再计算损失值，***线性损失函数值越代表损失值越小，效果越好，训练的过程就是不断的调整函数使得损失值不断逼近0***

  - 损失函数=数据损失+正则化惩罚项   L= {......}+λR(W)

  - 正则化惩罚项：R(W)=∑<sub>k</sub>∑<sub>l</sub>W<sup>2</sup><sub>k,l</sub> ，只考虑权重参数W

    

    ![image-20240511003804340](D:\postgraduate\deeplearing\images\前向传播)

- **Softmax分类器**：不同于线性函数的得分函数得出的是一个值，可以利用函数将得分值转化成概率，再使用概率计算损失值

  * ![image-20240511005000834](D:\postgraduate\deeplearing\images\归一化)
  * 映射到[0,1]之间之后，得到的概率越大表明损失越小，最后再计算损失值，概率越接近1，计算的损失值越接近0，损失越小

###### 前向传播

- 通过W，x的值计算出损失值，可能不只一层，每层都计算W<sub>i</sub>X<sub>i</sub>
- **反向传播**：通过损失值去更新调整模型（<mark>求新的W</mark>），是的损失值越来越低（梯度下降）
- 梯度下降：寻找山谷的最低点，也是函数的终点（使函数达到极值点），找到合适方向，一步步更新参数![image-20240511021029630](D:\postgraduate\deeplearing\images\梯度下降目标函数)
- 反向传播遵循链式法则，从后往前一步一步求偏导，实质就是求偏导，计算该输入因子对结果做的贡献即影响，要一步一步求，如：f=(x+y)*z,先求对（x+y）整体的偏导，再计算对x,y的偏导。(((xw<sub>1</sub>)w<sub>2</sub>)w<sub>3</sub>)=f，先求w3的影响，再求为w2的影响，再求w1的影响。（+1）->1/x，箭头上方是x的值，下方是通过后面的操作的偏导数在此处的值，倒着逐层计算的偏导是相乘的。
- 门单元：
- 加法门单元：均等分配，x+y=q，q对x，y求偏导，都为1，因此梯度是均匀的
  - MAX门单元：给最大的，将梯度往之前的方向传播给最大的，其他的不传
- 乘法门单元：互换的感觉，x*y=q，对x，y求偏导，为另一个值，因此有互换的感

## 神经网络

### 架构

![image-20240511234101936](D:\postgraduate\deeplearing\images\整体架构)

多少个圈代表输入了多少特征，如32\*32\*3的图片，有3072个圈，就是x（x个特征）\*1的矩阵（一组数据）（1batch就是一个一个样本，x个特征的单个样本） ，中间隐藏层是通过wx得来的矩阵，根据需要给出权重参数，<mark>中间的w操作进行反向传播时要求偏导</mark>，非线性：<mark>在w操作之后要通过使用非线性函数（如sigmmd函数，max函数）进行变换</mark>，eg：f=w3max(w2max(0,w1x),0 )，神经网络的强大之处在于可以使用更多的参数来拟合复杂的数据

**反向传播：求得使用函数对W求偏导，确定并调整w的权重，在神经元中也应该使用激活函数对相应的值求偏导，但是不更新值，这样一层一层往前传**

![image-20240511233909714](D:\postgraduate\deeplearing\images\神经网络)

##### 正则化的作用 

惩罚项中λ越大，数据会过拟合，我们尽量使得不那么过拟合把任务完成

##### 神经元

其中权重参数个数越多会过拟合，同正则化，尽量不那么拟合化完成任务

##### 激活函数

在wx之后进行的非线性变换使用的函数，如Sigmoid（1/（1+e<sup>-z</sup>））（可能由于某些层而导致计算出来的梯度为0，出现梯度消失的现象，从而不进行更新），Relu（max（0，x））（解决Sigmoid函数梯度消失问题），Ranh等函数

##### 数据预处理、参数初始化

常见的标准化    通常使用随机策略进行参数初始化

##### 如何解决神经网络中的过拟合问题

- ***DROP-OUT***：随机在每一层 的神经元中选取一定比例的神经元杀死（这次训练不使用）
- ***加正则化***

##### 学习率(Learning rate)

- 当学习率设置的**过小**时，**收敛过程将变得十分缓慢**。而当学习率设置的**过大**时，**梯度可能会在最小值附近来回震荡，甚至可能无法收敛**。
- 固定学习率时，当到达收敛状态时，会在最优值附近一个**较大的区域内**摆动；而当随着迭代轮次的增加而减小学习率，会使得在收敛时，在最优值附近一个**更小的区域**内摆动。（之所以曲线震荡朝向最优值收敛，是因为在每一个mini-batch中都存在噪音）。

###### 学习率的调整 ：控制权重更新到l最小点的速度

- **离散下降（discrete staircase）**:对于**深度学习**来说，每 t𝑡 轮学习，学习率减半。对于**监督学习**来说，初始设置一个较大的学习率，然后随着迭代次数的增加，减小学习率。
- **指数减缓（exponential decay）**: 对于深度学习来说，学习率按训练轮数增长指数差值递减。如：
  - **α=0.95<sup>epch_num</sup>\*α<sub>0</sub>  **
  - 或者： *** α=k/epoch_num<sup>1/2 </sup>***，epoch_num为迭代次数
- **分数减缓（1/t decay）**:  对于**深度学习**来说，学习率按照公式 **α=α/(1+decay_rate\*epoch_num)** 变化， decay_rate控制减缓幅度。

##### 深度学习中常用的损失函数

- **均方差损失 Mean Squared Error Loss（L2  Loss）**:常用于**回归任务**

  ![image-20240530225721406](D:\postgraduate\deeplearing\images\MSELoss)

- **平均绝对误差损失 Mean Absolute Error Loss(L1  Loss)**：常用于**回归任务**

  ![image-20240530225844661](D:\postgraduate\deeplearing\images\MAELoss)

  **MSE 通常比 MAE 可以更快地收敛**，**MAE 对于 outlier 更加 robust**

- **Huber Loss**：在误差接近 0 时使用 MSE，误差较大时使用 MAE

  ![image-20240530230015124](D:\postgraduate\deeplearing\images\SMAELoss)

- **分位数损失 Quantile Loss**：**回归算法**，**分别用不同的系数控制高估和低估的损失，进而实现分位数回归**

  ![image-20240530230107855](D:\postgraduate\deeplearing\images\分数位损失)

- **交叉熵损失 Cross Entropy Loss**：**分类任务**
  - 二分类：考虑二分类，在二分类中我们通常使用 Sigmoid 函数将模型的输出压缩到 (0, 1) 区间内 ，用来代表给定输入 ，模型判断为正类的概率。![image-20240530230346772](D:\postgraduate\deeplearing\images\二分类交叉熵)
  - 多分类：![image-20240530230450316](D:\postgraduate\deeplearing\images\多分类交叉熵)

- **合页损失 Hinge Loss**：合页损失 Hinge Loss 是另外一种二分类损失函数，适用于 **maximum-margin 的分类**，支持向量机 Support Vector Machine (SVM) 模型的损失函数本质上就是 Hinge Loss + L2 正则化

- train loss 不断下降，test loss不断下降，说明网络仍在学习;
  train loss 不断下降，test loss趋于不变，说明网络过拟合;
  train loss 趋于不变，test loss不断下降，说明数据集100%有问题;
  train loss 趋于不变，test loss趋于不变，说明学习遇到瓶颈，需要减小学习率或批量数目;
  train loss 不断上升，test loss不断上升，说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题。

##### 梯度下降 

###### 梯度：梯度是一个向量，用来指明在函数的某一点，沿着哪个方向函数值上升最快，这个向量的模指明函数值上升程度（速度）的大小。

其梯度是![在这里插入图片描述](https://img-blog.csdnimg.cn/20190805142545814.png)，在定义一个学习率n，来定义每次更新的幅度，也可以说是每次参数移动的幅度。由此得到参数的更新公式：![在这里插入图片描述](D:\postgraduate\deeplearing\images\权重参数调整公式)

梯度下降学习算法有三种类型：批量梯度下降、随机梯度下降和小批量梯度下降。 

###### 批量梯度下降算法   

批量梯度下降算法对训练集中每个点的误差求总和， 只有在所有训练示例都评估后才更新模型。  这个过程称为一个训练周期 (training epoch)。

虽然这种批量处理提高了计算效率，但对于大型训练数据集而言，它仍然需要很长的处理时间，因为仍要将所有数据存储到内存中。 批量梯度下降算法通常也会产生稳定的误差梯度和收敛性，但有时在寻找局部最小值和全局最小值时，收敛点并不是最理想。

###### 随机梯度下降算法   

随机梯度下降算法 (SGD) 为数据集中的每个示例运行一个训练周期，并一次性更新所有训练示例的参数。 由于只需保存一个训练示例，所以可以更轻松地将它们存储在内存中。 虽然这些频繁的更新可以使计算更加详细，速度更快，但与批量梯度下降算法相比，这可能会导致计算效率下降。 随机梯度下降算法的频繁更新可能导致嘈杂梯度，但这也有助于避开局部最小值，找到全局最小值。

###### 小批量梯度下降算法   

小批量梯度下降算法结合了批量梯度下降算法和随机梯度下降算法的理念。  它将训练数据集分成小批次， 并对每批进行更新。  这种方法兼顾了批量梯度下降算法的计算效率和随机梯度下降算法的速度。  

## 卷积神经网络（CNN）

##### 应用

* 检测任务：识别图像等

- 分类与检索：识别一个图片是什么，通过输入的图像，输出相似的图片
- 超分辨率重构：将输入的图像变得更见清晰
- 医学任务
- 无人驾驶
- 人脸识别

##### 卷积神经网络和传统神经网络的区别：

![image-20240512160316046](D:\postgraduate\deeplearing\images\卷积神经网络和传统神经网络的区别)

- 卷积神经网络输入的是三维的，立方体，输入10\*10\*10  h\*w\*c
- 传统神经网络输入的是二维，如有1000个特征，有1000个输入，通过一组权重参数计算出来特征值

##### 整体架构

![image-20240512203747313](D:\postgraduate\deeplearing\images\卷积神经整体架构)

- 输入层：输入

- 卷积层：提取特征  将输入分成多个小的区域，对每个区域求特征值（通过过滤矩阵求内积计算出特征值（得分值）），最后求出符合的整体的特征值。（相当于该区域是一个滑动窗口，对每个窗口内部分求特征值，这样向后/下滑动设置的步长，求出每个特征值）ps: 图像输入的话对每个颜色通道的二维矩阵都要计算一遍特征值，中间使用的辅助矩阵w也应该有对应颜色通道个数个，<mark>求出该矩阵对应区域不同颜色通道的特征值之和再加上偏置值，就是该区域(使用滑动窗口确定的区域)的特征值</mark>，求出来的对应总体区域的特征表通常被称为feature map(特征图)，该特征图大小可以设置来表示整个图，可以使用多个辅助矩阵计算特征值求出多个feature map

-  一个卷积核代表一层，对于w\*h\*C的数据，使用C\*w1*h1的卷积核，C是对于每个颜色通道由每隔w1\*h1矩阵进行相乘计算，每一个求出来之后相加一起得到特征图，最后一层卷积对应一个偏置值要加上偏置，之后经过激活函数激活，最后是全连接层

  输入的时候输入h(height)\*w(width)\*c(channel)，输出的时候c对应个数，表示特征图的个数

  ![image-20240512232405096](D:\postgraduate\deeplearing\images\卷积层)

  - 卷积层可以有多个，理论上可以无限堆叠，一般有限制，一般卷积层有以下参数：

    - 滑动窗口步长：步长越小特征值划分的越精细，需要的时间越长 
    - 卷积核尺寸：就是选择的辅助矩阵的区域大小
    - 边缘填充：为了能使输入数据靠近边缘的地方尽可能的更多次的被利用，可以再边缘添加一层数据0（Zero-Padding）
    - 卷积核个数：得到的特征图的个数

  - 卷积结果计算公式： (h1+2\*p+1-F)/s向上取整

    ![image-20240513000157888](D:\postgraduate\deeplearing\images\卷积结果计算公式)

  - 卷积参数共享：对每个滑动窗口都选用相同的卷积核，可以有多个卷积核，每个卷积核都有一个对应的偏置参数

- 激活函数：激活函数（Activation Function）是一种添加到人工神经网络中的函数，**旨在帮助网络学习数据中的复杂模式**。 类似于人类大脑中基于神经元的模型，激活函数最终决定了要发射给下一个神经元的内容。 在人工神经网络中，一个节点的激活函数定义了该节点在给定的输入或输入集合下的输出，**为了增加模型的非线性**

- 池化层：压缩特征 ，不涉及任何矩阵运算，只有筛选

  - 最大池化（MAX  POOLING）：从特定区域选出最大的特征值
  - 平均池化（AVERAGE  POOLING）: 现在一般不适用

- 卷积、激活、池化的顺序

  - 如果使用**最大池化**，两种顺序的结果是一样的。
  - 如果使用**平均池化**，建议采用第1种顺序，即**卷积 -> 激活 -> 池化**。

- 全连接层：由于卷积之后得到的特征图是立体的，要想完成分类任务等，要通过一组权重参数和输入的连接，计算得分

  - 全连接层 Fully Connected Layer 一般位于整个卷积神经网络的最后，负责将卷积输出的二维特征图转化成**一维的一个向量**，由此实现了端到端的学习过程（即：输入一张图像或一段语音，输出一个向量或信息）。全连接层的每一个结点都与上一层的所有结点相连因而称之为全连接层。由于其全相连的特性，一般全连接层的参数也是最多的。
  - **全连接层的主要作用就是将前层（卷积、池化等层）计算得到的特征空间映射样本标记空间。简单的说就是将特征表示整合成一个值，其优点在于减少特征位置对于分类结果的影响，提高了整个网络的鲁棒性。**

- **在全连接层之前基本只是特征提取，利用反向传播更新权重w的值以及偏置b，在全连接层将权重和输入相乘并加上偏置值得出一组一维向量，softmax是将结果映射成概率，多用于分类任务，其他的可以用于线性回归**

- **全连接层**

  [全连接层](https://so.csdn.net/so/search?q=全连接层&spm=1001.2101.3001.7020)的输入是一维数组，多维数组需先进行Flatten进行一维化处理，然后连接全连接层。全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。全连接图结构如下。

  ![全连接层的图结构](D:\postgraduate\deeplearing\images\全连接层)

  其中，x1、x2、x3为全连接层的输入，a1、a2、a3为输出，有![在这里插入图片描述](https://img-blog.csdnimg.cn/20190307123227154.png)

  ##### **全连接层参数计算**

  权值参数=输入[一维数组](https://so.csdn.net/so/search?q=一维数组&spm=1001.2101.3001.7020)大小*全连接层输出结点数
  偏置参数b=全连接层输出结点数

  **eg:**
  输入有[50*4*4]个神经元结点，输出有500个结点，则一共需要50*4*4*500=400000个权值参数W和500个偏置参数b

  ##### **卷积和全连接层**

  个人感觉，卷积跟全连接都是一个点乘的操作，区别在于卷积是作用在一个局部的区域，而全连接是对于整个输入而言，那么只要把卷积作用的区域扩大为整个输入，那就变成全连接了，我就不给出形式化定义了。所以我们只需要**把卷积核变成跟输入的一个map的大小一样**就可以了，这样的话就相当于使得卷积跟全连接层的参数一样多。
  **eg**: 

  - **传统方法**：输入是224x224x3 的图像，假设经过变换之后最后一层是[7x7x512]的，那么传统的方法应该将其展平成为一个**7x7**x512长度的一层，然后做全连接层，假设全连接层为4096×1000层的（假设有1000个分类结果）。

  - **使用1×1卷积核**：因为1×1卷积核相当于在不同channel之间做线性变换，所以：先选择**7×7**的卷积核（这是对7\*7*512的卷积结果进行的操作），输出层特征层数为4096层（自己选择的4090个核，结果会有4090个特征图），这样得到一个[1×1×4096]层的
    然后再选择用1×1卷积核，输出层数为1000层（由于要输出1000个分类结果，因此有1000个特征图，所以用1000个卷积核），这样得到一个[1×1×1000]层这样就搞定了。

  **总结**：全连接的结果就是一个1\*要输出的类别个数的一组向量，可以使用：

  - 传统方法：将全连接层前面一层的卷积结果平铺成一个一维的向量，这一维向量相当于x，是最后w\*x+b中的x，然后进行全连接，全连接层对于所有的神经元输入要有一组对应个数的权重参数，由于有n类输入，所以要有n\*x个权重参数，其中对于一组要有偏置值，所以应该有n\*x+b个参数，最后的结果是n\*1的向量
  - 使用1\*1的卷积核：假设上一层结果是h\*w\*channel的特征图，可以用m个h*w大小的卷积核，运算之后得到1\*1\*m的特征图，再使用n个1\*1的卷积核运算卷积之后得到1\*n的一组输出向量

##### 图像颜色通道 

![image-20240512211232157](D:\postgraduate\deeplearing\images\卷积中RGB通道)

- R channel  G channel B channel 计算的时候每个颜色通道都要分别进行计算（卷积层特征提取，有偏置的话还要加上偏置值），最后加在一起

##### 卷积神经网络的流程

convolution  Relu  convilution  Relu   Pool   convolution Relu  convolution Relu  Pool   两次卷积一次池化，不断提取特征  最后得到特征图  在经过转换处理将三维特征图拉成向量，再进行全连接Size:   向量个数\*分类个数。只有带参数（权重参数w，偏置b）参与计算才能算作神经网络的一层，Relu   pool   不算，convolution和FC算

![image-20240518000555964](D:\postgraduate\deeplearing\images\特征图变化)



##### 经典卷积网络

- Alexnet :  8层网路   5层的卷积   3层的全连接

- Vgg : 所有卷积大小都是3\*3的   有16/19层网络版本   当pooling之后损失特征信息，在下一次卷积中将特征图翻倍  相当于用特征图的个弥补pooling中损失的特征信息   与Alexnet相比训练所用的时间更长，效果比Alexnet更好  在卷积中得到的特征图不一定都是好的，因此当卷积层数越来越大的时候效果不一定会好 ，至此深度学习发展遇到瓶颈，当<mark>网络层数越来越大的时候错误率会更高</mark>

- **残差网络Resnet**

  为了解决神经网络层数越多错误率越高的问题，可用如下解决方案：  在卷积之后加上原来的，这样有原来的值进行保底，<mark>前两层卷积的效果如果不好就可以调整权重参数为0，这样即使卷积的结果为0还有原值保底</mark>，

  测试之后层数越多损失率越低，只是层数过多之后提升不明显，在竞赛或者学术中公认比较好的，可以优先选用。56层/101层    可以用于各种检索，分类，识别，<mark>分类还是回归任务取决于损失函数以及连接</mark>

  ![image-20240518123108935](D:\postgraduate\deeplearing\images\突破神经网络发展的瓶颈)

    ##### 感受野 

感受野就是得到当前的/最后的值是前面参与计算的所有值的个数/大小，所以对于当前值，感受野越大越好，即参与计算的值的个数越多代表经过在各种干扰之下，各种特征提取到，筛选的特征越好，因此感受野越大越好

一个3\*3的卷积层就是一个3\*3的卷积核，对于一个单元，经过反向观察，每个单元对应能感受到的原始图像的大小就是感受野

- 好处：堆叠小的卷积核所需的参数更少一些，并且卷积过程中特征提取也会越细致，加入的非线性变换也随之增多（一次卷积完成加一次Relu），还不会增加权重参数的个数，这就是VGG网络的基本出发点，用小的卷积核来完成体特征的提取操作。

  * 假设输入大小为h\*w\*C，并且使用 C个卷积核（得到c个特征图）

  - 一个7\*7卷积核所需要的参数：C\*(7\*7\*C)=49C<sup>2 </sup>  7\*7\*C为C个7\*7大小的卷积，使用C个再乘以C
  - 三个3\*3卷积核需要的参数：3\*C\*(3*3\*C)=27C<sup>2</sup>



![image-20240518155604352](D:\postgraduate\deeplearing\images\感受野)

## 递归神经网络（RNN）

##### RNN

其中当前时序状态下的隐藏层的得到的特征值是从上一个时序状态得来的

- 卷积神经网络（CNN）主要用于CV（computer vision/计算机视觉）

- 递归神经网络（RNN）主要用于NLP（Natural Language Processing/自然语言处理）
- 缺点：前面的所有时序中有可能有的时序状态的记录不重要，可以舍弃，经典RNN中不舍弃，全部记忆

![image-20240518212052040](D:\postgraduate\deeplearing\images\RNN)

![image-20240518222930351](D:\postgraduate\deeplearing\images\递归神经网络)

##### LSTM(长短期记忆)

- sigmoid函数输出0~1之间的数，0代表不允许任何量通过，1代表允许任意量通过

![image-20240518223549991](D:\postgraduate\deeplearing\images\lstm)

![image-20240518223910992](D:\postgraduate\deeplearing\images\lstm-1)

![image-20240518223940581](D:\postgraduate\deeplearing\images\lstm-2)

## 自然语言处理-词向量模型-Word2Vec

##### 将文本向量化

 将文本转化成多维向量，对向量进行操作，训练

在构建的词库中进行查找文本中词的向量，初始时随机初始化词库，再通过反向传播进行更新

##### 构建训练模型

将训练用的文本通过设置一个滑动窗口进行输入输出的设计，进行训练

![image-20240518235934011](D:\postgraduate\deeplearing\images\NLP构建训练模型)

##### 模型选择

- CBOW: 知道前后的词，对中间的词进行输入
- Skip-gram: 知道中间的词，对左右的词进行预测

###### 出现的问题

- 如果一个语料库稍微大一些，可能的结果很多，最后一层softmax计算起来非常耗费时间，如何解决：

  - 初始方案：输入两个单词，看他们前后对应的输入和输出，相当于一个二分类任务

  - 缺点：此时训练集构建出来的标签全部为1，无法进行比较好的训练

  - 改进方案：加入一些负样本（负样本模型）-人为的创建target为0的样本

    ![image-20240519002915154](D:\postgraduate\deeplearing\images\NLP文本数据模型构建改进)

##### 词向量训练过程

- 初始化词向量矩阵

  ![image-20240519003047399](D:\postgraduate\deeplearing\images\初始化词向量矩阵)

  ![image-20240519003553457](D:\postgraduate\deeplearing\images\初始化向量矩阵-2)

- 通过神经网络反向传播来计算更新，此时不止更新权重参数矩阵W，也要更新输入数据

  ![image-20240519003828174](D:\postgraduate\deeplearing\images\NLP训练中更新参数)

- 训练完毕后最终得到每个词对应的向量表
